{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -U tqdm\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "10/10 [==============================] - 0s 36ms/sample - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "\n",
    "def get_run_logdir(prefix=\"\"):\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\" + prefix)\n",
    "    return os.path.join(\"/home/infodba/Documents/study/hands_on_ml2/my_logs/\", run_id)\n",
    "\n",
    "test_x = np.arange(10)\n",
    "test_y = np.zeros((10,1))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[1]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(test_x, test_y, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow like NumPy\n",
    "\n",
    "### Tensors and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=42>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix\n",
    "t2 = tf.constant(42) # scalar\n",
    "t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 3]), tf.float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape, t1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[:, 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = t1+10 #equivalent tf.add(t1, 10)\n",
    "t4 = tf.square(t1)\n",
    "t5 = tf.transpose(t1)\n",
    "t6 = t1 @ t5 #equivalent np.matmul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=21.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=21.0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_sum(t1), tf.reduce_sum(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[1., 4.],\n",
       "       [2., 5.],\n",
       "       [3., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = tf.transpose(t1)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = t @ t3\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = tf.matmul(tf.transpose(t3), t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\n",
       "array([[1, 2, 3, 1, 2, 3],\n",
       "       [4, 5, 6, 4, 5, 6],\n",
       "       [1, 2, 3, 1, 2, 3],\n",
       "       [4, 5, 6, 4, 5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1,2,3],[4,5,6]], tf.int32)\n",
    "b = tf.constant([2,2], tf.int32)\n",
    "tf.tile(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример вызова low level API из keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[11., 26.],\n",
       "       [14., 35.],\n",
       "       [19., 46.]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "K = keras.backend\n",
    "\n",
    "K.square(K.transpose(t1)) + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float16, numpy=array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float16)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.arange(10, dtype=np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10, dtype=tf.float64).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81], dtype=int32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(tf.range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: add/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-f0e87383d745>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: add/"
     ]
    }
   ],
   "source": [
    "tf.constant(2.) + tf.constant(40)\n",
    "tf.constant(2.) + tf.constant(40, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=40.0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = tf.constant(2.) \n",
    "t2 = tf.constant(40, dtype=tf.float64)\n",
    "tf.cast(t2, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = tf.Variable([[1.,2.,3.], [4.,5.,6.]])\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.assign(2*v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1[0,1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1[:,2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.scatter_nd_update(indices=[[0,0], [1,2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Models and Training Algorithms\n",
    "\n",
    "### Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "10/10 [==============================] - 1s 59ms/sample - loss: 0.3823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbf9879d358>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "model.fit(test_x, test_y, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models That Contain Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model.h5\", custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо реализовать функцию потерь с аргументами, то при показанном выше подходе аргументы не будут сохраняться. Чтобы сохранять аргументы функции потерь необходимо делать ее реализацию на базе класса keras.losses.Loss (см. 377).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Activation Functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): #line tf.nn.softplus()\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01*weights))\n",
    "    \n",
    "def my_positive_weights(weights): #is as tf.nn.relu()\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights),  weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(activation=my_softplus, units=30,\n",
    "                          kernel_initializer=my_l1_regularizer, \n",
    "                          kernel_regularizer=my_l1_regularizer,\n",
    "                          kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 1s 63ms/sample - loss: 0.0625 - huber_fn: 0.0313\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 3ms/sample - loss: 3.2970 - huber_fn: 1.1086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbfa0c52668>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[huber_fn])\n",
    "model.fit(test_x, test_y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особый случай с бинарной классификацией (см. на стр. 381).\n",
    "\n",
    "Например, 1й проход дал 5 позитивных предсказания из которых 1 ложно позитивный, итоговая точность 4/5 = 80%; 2й проход дал 3 ложно положительных, итоговая точность 0%. Т.о. средняя точность получается 40%, но это не верно, т.к. (4+0)/(5+3) = 50%.  Для решения задачи используется keras.metrics.Precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "\n",
    "#pass 1 batch of predictions and lables for binary classification task\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pass 2 (emulation the next pass)\n",
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример собственной реализации потоковой метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = huber_fn\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        #metric = self.huber_fn(y_true, y_pred)\n",
    "        metric = y_true - y_pred\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "        print(self.total)\n",
    "        print(self.count)\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self): #used to save threshold while model is saving \n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 1s 70ms/sample - loss: 1.0066 - huber_metric_31: 0.3496 \n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 8ms/sample - loss: 0.0252 - huber_metric_31: 0.0126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbfa0052c50>"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = tf.range(10, dtype=tf.float32)\n",
    "test_y = tf.zeros(10)\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[HuberMetric(2.0)])\n",
    "model.fit(test_x, test_y, epochs=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples\n",
      "10/10 [==============================] - 1s 54ms/sample - loss: 7.4998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb588127128>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[1]),\n",
    "    keras.layers.Dense(1),\n",
    "    exp_layer\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(test_x, test_y, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        #print(\"in build() method\")\n",
    "        #print(\"batch_input_shape:\", batch_input_shape)\n",
    "        #print(batch_input_shape.as_list()[:-1])\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        #print(\"in call() method\")\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        a = tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "        print(\"compute_output_shape() = \", a)\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.activations.get(\"relu\") #test, just interesting to look up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(10, activation=\"relu\", input_shape=[10]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо обеспечить различное поведение слоев в процессе обучения и тестирования (полезно в случаях, например, Dropout или BN), то  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else: return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_layers = n_layers                                   \n",
    "        self.n_neurons = n_neurons                                  \n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "    \n",
    "    def get_config(self):                                               \n",
    "        base_config = super().get_config()                              \n",
    "        return {**base_config,                                          \n",
    "                \"n_layers\": self.n_layers, \"n_neurons\": self.n_neurons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim                                 \n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)\n",
    "\n",
    "    def get_config(self):                                           \n",
    "        base_config = super().get_config()                           \n",
    "        return {**base_config,                                     \n",
    "                \"output_dim\": self.output_dim}                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        # TODO: check https://github.com/tensorflow/tensorflow/issues/26260\n",
    "        #self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        #if training:\n",
    "        #    result = self.reconstruction_mean(recon_loss)\n",
    "        #    self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients Using Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3*w1**2 + 2*w1*w2\n",
    "\n",
    "def print_f_derivative(w1, w2):\n",
    "    w1_dir = 6*w1 + 2*w2\n",
    "    w2_dir = 2*w1\n",
    "    print(\"w1 partial derivative:\", w1_dir)\n",
    "    print(\"w2 partial derivative:\", w2_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 partial derivative: 66\n",
      "w2 partial derivative: 20\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = 10,3\n",
    "print_f_derivative(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 partial derivative: 36.000003007075065\n",
      "w2 partial derivative: 10.000000003174137\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6\n",
    "w1_dir = (f(w1 + eps, w2) - f(w1, w2)) / eps\n",
    "w2_dir = (f(w1, w2 + eps) - f(w1, w2)) / eps\n",
    "\n",
    "print(\"w1 partial derivative:\", w1_dir)\n",
    "print(\"w2 partial derivative:\", w2_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "#gradients2 = tape.gradient(z, [w1, w2]) #RuntimeError here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approach if you want to call gradient() twice or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dw1 = tape.gradient(z, w1)\n",
    "dw2 = tape.gradient(z, w2)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw1, dw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tape object traks only tf.Variable. It doesn't work for another types like tf.constant for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it has an approach for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "    \n",
    "tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the gradient of a list of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2)\n",
    "    z2 = f(w1, w2)\n",
    "    z3 = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient([z1, z2, z3], [w1, w2])\n",
    "#gradients2 = tape.gradient(z, [w1, w2]) #RuntimeError here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=108.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients #contains the sum of gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want to compute gradients separately we mast execute gradients() for each tensor and use persistent tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z1 = f(w1, w2)\n",
    "    z2 = f(w1, w2)\n",
    "    z3 = f(w1, w2)\n",
    "\n",
    "gw1 = tape.gradient(z1, [w1, w2])\n",
    "gw2 = tape.gradient(z2, [w1, w2])\n",
    "gw3 = tape.gradient(z3, [w1, w2])\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=10.0>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=10.0>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=10.0>])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gw1, gw2, gw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of partial defivatives of the partial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = f(w1, w2)\n",
    "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
    "hessians = [hessian_tape.gradient(jacobian, [w1, w2])\n",
    "           for jacobian in jacobians]\n",
    "del hessian_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz_dw1_dw1: tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "dz_dw1_dw2: tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "dz_dw2_dw1: tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "dz_dw2_dw2: None\n"
     ]
    }
   ],
   "source": [
    "print(\"dz_dw1_dw1:\", hessians[0][0])\n",
    "print(\"dz_dw1_dw2:\", hessians[0][1])\n",
    "print(\"dz_dw2_dw1:\", hessians[1][0])\n",
    "print(\"dz_dw2_dw2:\", hessians[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если необходимо не вычислять градиенты для части нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f2(w1, w2):\n",
    "    return 3*w1**2 + tf.stop_gradient(2*w1*w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f2(w1, w2)\n",
    "    \n",
    "tape.gradient(z, [w1,w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show some numerical problems related computing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): #line tf.nn.softplus()\n",
    "    return tf.math.log(tf.exp(z) + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([inf], dtype=float32)>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.Variable([100.])\n",
    "\n",
    "my_softplus(w) # the result overloads the type capasity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# due to it the result of calulating gradients became NAN\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(w)\n",
    "\n",
    "tape.gradient(z, [w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to resolve that issue we can manualy find the derivative of my_softmax that is just 1/(1 + 1/exp(x)). Next we tell TF to use our derivative with tf.custom_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5000125], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad/(1+ 1/exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(w)\n",
    "    \n",
    "print(tape.gradient(z, [w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function my_softplus_gradients receive as argument 'grad' that where backpropagated so far, down to the softplus function. According chain rule we should multiply them with this function's gradients.\n",
    "\n",
    "\n",
    "### Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", \n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                      kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(x, y, batch_size=32):\n",
    "    idx = np.random.randint(len(x), size=batch_size)\n",
    "    return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's build the custom loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Layer dense_34 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "11610/11610 - mean: 1.4430 - mean_absolute_error: 0.5821\n",
      "Epoch 2/5\n",
      "11610/11610 - mean: 0.6749 - mean_absolute_error: 0.5240\n",
      "Epoch 3/5\n",
      "11610/11610 - mean: 0.6416 - mean_absolute_error: 0.5140\n",
      "Epoch 4/5\n",
      "11610/11610 - mean: 0.6337 - mean_absolute_error: 0.5143\n",
      "Epoch 5/5\n",
      "11610/11610 - mean: 0.6640 - mean_absolute_error: 0.5249\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Functions and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(a):\n",
    "    return a**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useful things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.0,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=8.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=8.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=8.0>,\n",
       " <tensorflow.python.eager.def_function.Function at 0x7fb5303de160>)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto1 = cube(2.)\n",
    "auto2 = cube(tf.constant(2.))\n",
    "func1 = tf.function(cube)\n",
    "auto3 = func1(tf.constant(2.))\n",
    "auto4 = func1(2.)\n",
    "auto1, auto2, auto3, auto4, func1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(a):\n",
    "    return a**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cube(a):\n",
    "    return a**3\n",
    "\n",
    "tf_func = tf.function(cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1000, 8000], dtype=int32)>"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(10))       #1\n",
    "tf_cube(tf.constant(20))       #2\n",
    "tf_cube(tf.constant([10, 20])) #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def tf__tf_cube(a):\\n  do_return = False\\n  retval_ = ag__.UndefinedReturnValue()\\n  with ag__.FunctionScope('tf_cube', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\\n    do_return = True\\n    retval_ = fscope.mark_return_value(a ** 3)\\n  do_return,\\n  return ag__.retval(retval_)\\n\""
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.autograph.to_code(tf_cube.python_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "### task1\n",
    "\n",
    "*How would you describe TensorFlow in a short sentence? What are its main features? Can you name other popular Deep Learning libraries?*\n",
    "\n",
    "TF - это фреймвор, предоставляющий мощное API для математических вычислений (аналогично numpy), адаптированное на вычисления в области машинного обучения, включая оптимизацию вычислений, использование GPU, возмжоность запуска обученной модели на другой оборудовании и т.п. Другие популярные библиотеки: Theano,  Microsoft Cognitive Toolkit (CNTK), MXNet, Caffe2, Chainer.\n",
    "\n",
    "### task2 \n",
    "\n",
    "*Is TensorFlow a drop-in replacement for NumPy? What are the main differences between the two?*\n",
    "\n",
    "Хоть TF и реализут большинство фукнциональности NumPy, но:\n",
    "- названия функций отличаются;\n",
    "- поведения фукнций отличаются, т.к. функции TF реализованы так, чтобы выполняться многопоточно, либо на GPU. Например tf.reduce_sum против np.sum или например, транспонирование в numpy фактически не осуществялется, а только предоставляет возможность обращаться к матрица, так, как если бы она была транспонирована, а вот в tf осуществляется фактически.\n",
    "- TF имеет константные тензора, которые не могут изменяться.\n",
    "\n",
    "### task3\n",
    "\n",
    "*Do you get the same result with tf.range(10) and tf.constant( np.arange(10))?*\n",
    "\n",
    "Нет, 1й возвращает тензор с типом int32, второй int64\n",
    "\n",
    "### task4\n",
    "\n",
    "*Can you name six other data structures available in TensorFlow, beyond regular tensors?*\n",
    "\n",
    "- tf.SparseTensor\n",
    "- tf.TensorArray\n",
    "- tf.RaggedTensor\n",
    "- tf.string\n",
    "- tf.sets \n",
    "- tf.queue\n",
    "\n",
    "### task5\n",
    "\n",
    "*A custom loss function can be defined by writing a function or by subclassing the keras.losses.Loss class. When would you use each option?*\n",
    "\n",
    "Наследование keras.losses.Loss используется, когда подсчет потерь на базе предсказанного значения и значения label недостаточно и нужно использовать гиперпараметры. Так же наследование дает больше возможностей при сохранении и загрузки модели.\n",
    "\n",
    "### task 6\n",
    "\n",
    "*Similarly, a custom metric can be defined in a function or a subclass of keras.metrics.Metric. When would you use each option?*\n",
    "\n",
    "Аналогично ответу 5. \n",
    "\n",
    "Moreover, if computing the metric over a whole epoch is not equivalent to computing the mean metric over all batches in that epoch (e.g., as for the precision and recall metrics), then you should subclass the keras.metrics.Metric class and implement the __init__(), update_state(), and result() methods to keep track of a running metric during each epoch. You should also implement the reset_states() method unless all it needs to do is reset all variables to 0.0. If you want the state to be saved along with the model, then you should implement the get_config() method as well.\n",
    "\n",
    "### tasl 7\n",
    "\n",
    "*When should you create a custom layer versus a custom model?*\n",
    "\n",
    "В целом различие в технологии. Мы должны представить модель в целом и отделить то, что является слоями от того, что является моделью. Технически, с точки зрения библиотеки слои и модель почти одно и то же, только модель обладает соовтетствующими функциями (compile, fit...)\n",
    "\n",
    "You should distinguish the internal components of your model (i.e., layers or reusable blocks of layers) from the model itself (i.e., the object you will train). The former should subclass the keras.layers.Layer class, while the latter should subclass the keras.models.Model class.\n",
    "\n",
    "### task 8\n",
    "\n",
    "*What are some use cases that require writing your own custom training loop?*\n",
    "\n",
    "Получить полный контроль за ходом обучения, чтобы быть уверенным что все выполняется с точностью так, как запланировано. Осуществить оладку, выводя доп. информацию в процессе обучения. Керас предоставляет много возможностей по реализации различных задач без необходимости выполнять custom loop: обратные вызовы, пользовательская регуляризация, пользовательские фукнции потерь и т.п., поэтому рекомендуется этого не делать, т.к. этот подход более подвержен ошибкам.\n",
    "\n",
    "However, in some cases writing a custom training loop is necessary—for example, if you want to use different optimizers for different parts of your neural network, like in the Wide & Deep paper\n",
    "\n",
    "### tasl 9\n",
    "\n",
    "*Can custom Keras components contain arbitrary Python code, or must they be convertible to TF Functions?*\n",
    "\n",
    "Необходимо конвертиться в TF фукнции, т.к. TF насыщает их TF операциями, которые могут быть использованы для построения и последующей оптимизации графа вычислений. Однако есть пути использования обычных операций Python/Numpy.\n",
    "\n",
    "If you absolutely need to include arbitrary Python code in a custom component, you can either wrap it in a tf.py_function() operation (but this will reduce performance and limit your model’s portability) or set dynamic=True when creating the custom layer or model (or set run_eagerly=True when calling the model’s compile() method).\n",
    "\n",
    "### task 10\n",
    "\n",
    "*What are the main rules to respect if you want a function to be convertible to a TF Function?*\n",
    "\n",
    "\n",
    "- Если используется вызов из внешней библиотеки (напр. Numpy), то такая фукнция будте вызываться как есть и не будет являться частью графа. Поэтому нужно убедиться, что используется tf.reduce_sum вместо np.sum и т.п. (подробнее на стр.  409) \n",
    "- TF function, если ей необходимо создать переменную, должна это сделать только при 1м вызове, иначе будет исключение. Создания переменных в фукнция нужно стараться избегать, лучше их создавать в слоях в методе build() ).  \n",
    "- исходный код фукнции должен быть доступен TF для парсинга.  \n",
    "- TF парсит циклы, которые выполняются по объектам тензора или dataset (напр. for i in tf.range(x)). \n",
    "- Для производительности следует предпочитать векторизованную реализацию, чем использовать циклы. \n",
    "\n",
    "\n",
    "### task11\n",
    "\n",
    "*When would you need to create a dynamic Keras model? How do you do that? Why not make all your models dynamic?*\n",
    "\n",
    "Creating a dynamic Keras model can be useful for debugging, as it will not compile any custom component to a TF Function, and you can use any Python debugger to debug your code. It can also be useful if you want to include arbitrary Python code in your model (or in your training code), including calls to external libraries. To make a model dynamic, you must set dynamic=True when creating it. Alternatively, you can set run_eagerly=True when calling the model’s compile() method. Making a model dynamic prevents Keras from using any of TensorFlow’s graph features, so it will slow down training and inference, and you will not have the possibility to export the computation graph, which will limit your model’s portability.\n",
    "\n",
    "### task 12\n",
    "\n",
    "*Implement a custom layer that performs Layer Normalization (we will use this type of layer in Chapter 15):*\n",
    "\n",
    "*a. The build() method should define two trainable weights α and β, both of shape input_shape[-1:] and data type tf.float32. α should be initialized with 1s, and β with 0s.*\n",
    "\n",
    "*b. The call() method should compute the mean μ and standard deviation σ of each instance’s features. For this, you can use tf.nn.moments(inputs, axes=-1, keepdims=True), which returns the mean μ and the variance σ2 of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return **α⊗(X - μ)/(σ + ε) + β**, where ⊗ represents itemwise multiplication and ε is a smoothing term (small constant to avoid division by zero, e.g., 0.001).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer12(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, batch_input_shape):\n",
    "        self.a = self.add_weight(name='a_weight', shape=batch_input_shape[-1:], dtype=tf.float32, initializer=\"ones\")\n",
    "        self.b = self.add_weight(name='b_weight', shape=batch_input_shape[-1:], dtype=tf.float32, initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        mean = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        std  = tf.math.reduce_std(inputs, axis=-1, keepdims=True)\n",
    "        calc = tf.math.subtract(inputs, mean)\n",
    "        std_ = tf.math.add(std, 0.001)\n",
    "        calc = tf.math.divide(calc, std_)\n",
    "        calc = tf.math.multiply(self.a, calc) \n",
    "        return tf.math.add(calc, self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*c. Ensure that your custom layer produces the same (or very nearly the same) output as the  keras.layers.LayerNormalization layer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "tf.keras.backend.clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.models.Sequential([keras.layers.LayerNormalization(input_shape=[8])])\n",
    "norm_data_ln = model1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential([Layer12(input_shape=[8])])\n",
    "norm_data_my = model2.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "4.3577916e-06\n"
     ]
    }
   ],
   "source": [
    "delta = norm_data_ln - norm_data_my\n",
    "delta = np.round(delta, 4)\n",
    "mse   = mean_squared_error(norm_data_ln, norm_data_my)\n",
    "mse = np.sqrt(mse)\n",
    "\n",
    "count = 0\n",
    "for row in delta:\n",
    "    for cell in row:\n",
    "        if cell != 0:\n",
    "            count+=1\n",
    "\n",
    "\n",
    "print(count)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Различия между полученным нормализованными данными не велики, чему свидетельствует значение переменной count для 11610*8 объектов. Так же расстояние межу нормализациями представлно в виде метрики rmse.\n",
    "\n",
    "### task13\n",
    "\n",
    "*Train a model using a custom training loop to tackle the Fashion MNIST dataset (see Chapter 10).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_full.astype(np.float32)\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    Layer12(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*a. Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(x, y, batch_size=32):\n",
    "    idx = np.random.randint(len(x), size=batch_size)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metric, val_loss=None, val_metric=None):\n",
    "    print(\"\\r{}/{}: loss - {:.4f}, accuracy - {:.4f}\".format(iteration, total, loss, metric), end=\"\")\n",
    "    if iteration == total:\n",
    "        print(\", validation loss - {:.4f}, accuracy - {:.4f}\".format(val_loss, val_metric), end=\"\\n\")\n",
    "        \n",
    "def m_accuracy(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=1)#, output_type=y_true.dtype)\n",
    "    counter = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if tf.math.equal(y_pred[i], y_true[i]):\n",
    "            counter = tf.add(counter, 1)\n",
    "    return tf.divide(counter, len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55000/55000: loss - 0.5117, accuracy - 0.8204, validation loss - 0.4154, accuracy - 0.8524\n",
      "Epoch 2/5\n",
      "55000/55000: loss - 0.4194, accuracy - 0.8512, validation loss - 0.4906, accuracy - 0.8408\n",
      "Epoch 3/5\n",
      "55000/55000: loss - 0.4058, accuracy - 0.8568, validation loss - 0.4089, accuracy - 0.8594\n",
      "Epoch 4/5\n",
      "55000/55000: loss - 0.3911, accuracy - 0.8649, validation loss - 0.4215, accuracy - 0.8612\n",
      "Epoch 5/5\n",
      "55000/55000: loss - 0.3901, accuracy - 0.8633, validation loss - 0.4068, accuracy - 0.8664\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // (batch_size)\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn  = keras.losses.SparseCategoricalCrossentropy()\n",
    "mean_loss = keras.metrics.Mean()\n",
    "mean_acc  = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    los_dic = []\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        mean_acc(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), \n",
    "                         mean_loss.result().numpy(), \n",
    "                         mean_acc.result().numpy())\n",
    "        \n",
    "    y_pred   = model(X_valid)\n",
    "    val_loss = loss_fn(y_valid, y_pred)\n",
    "    val_acc  = m_accuracy(y_valid, y_pred)\n",
    "    \n",
    "    print_status_bar(len(y_train), len(y_train), \n",
    "                     mean_loss.result().numpy(), \n",
    "                     mean_acc.result().numpy(), \n",
    "                     val_loss, val_acc)\n",
    "    \n",
    "    mean_loss.reset_states()\n",
    "    mean_acc.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*b. Try using a different optimizer with a different learning rate for the upper layers and the lower layers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer12/a_weight:0 (784,)\n",
      "layer12/b_weight:0 (784,)\n",
      "dense/kernel:0 (784, 100)\n",
      "dense/bias:0 (100,)\n",
      "dense_1/kernel:0 (100, 100)\n",
      "dense_1/bias:0 (100,)\n",
      "dense_2/kernel:0 (100, 10)\n",
      "dense_2/bias:0 (10,)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    Layer12(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "for w in model.trainable_variables:\n",
    "    print(w.name, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55000/55000: loss - 0.4550, accuracy - 0.8383, validation loss - 0.4095, accuracy - 0.8574\n",
      "Epoch 2/5\n",
      "55000/55000: loss - 0.3583, accuracy - 0.8720, validation loss - 0.4423, accuracy - 0.8580\n",
      "Epoch 3/5\n",
      "55000/55000: loss - 0.3281, accuracy - 0.8809, validation loss - 0.4146, accuracy - 0.8616\n",
      "Epoch 4/5\n",
      "55000/55000: loss - 0.3167, accuracy - 0.8839, validation loss - 0.3889, accuracy - 0.8736\n",
      "Epoch 5/5\n",
      "55000/55000: loss - 0.3068, accuracy - 0.8882, validation loss - 0.3916, accuracy - 0.8754\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // (batch_size)\n",
    "loss_fn  = keras.losses.SparseCategoricalCrossentropy()\n",
    "mean_loss = keras.metrics.Mean()\n",
    "mean_acc  = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "optimizer1 = keras.optimizers.Nadam(lr=0.01)\n",
    "optimizer2 = tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    los_dic = []\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        optimizer1.apply_gradients(zip(gradients[:4], model.trainable_variables[:4]))\n",
    "        optimizer2.apply_gradients(zip(gradients[4:], model.trainable_variables[4:]))\n",
    "        \n",
    "        mean_loss(loss)\n",
    "        mean_acc(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), \n",
    "                         mean_loss.result().numpy(), \n",
    "                         mean_acc.result().numpy())\n",
    "        \n",
    "    y_pred   = model(X_valid)\n",
    "    val_loss = loss_fn(y_valid, y_pred)\n",
    "    val_acc  = m_accuracy(y_valid, y_pred)\n",
    "    \n",
    "    print_status_bar(len(y_train), len(y_train), \n",
    "                     mean_loss.result().numpy(), \n",
    "                     mean_acc.result().numpy(), \n",
    "                     val_loss, val_acc)\n",
    "    \n",
    "    mean_loss.reset_states()\n",
    "    mean_acc.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не знаю корректный ли подход, который я использовал выше (вроде норм судя по ряду выполненных тестов). Но в учебных материалах использовался другой подход. Сравним результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "55000/55000: loss - 0.4550, accuracy - 0.8383, validation loss - 0.4095, accuracy - 0.8574\n",
      "Epoch 2/5\n",
      "55000/55000: loss - 0.3583, accuracy - 0.8720, validation loss - 0.4423, accuracy - 0.8580\n",
      "Epoch 3/5\n",
      "55000/55000: loss - 0.3281, accuracy - 0.8809, validation loss - 0.4146, accuracy - 0.8616\n",
      "Epoch 4/5\n",
      "55000/55000: loss - 0.3167, accuracy - 0.8839, validation loss - 0.3889, accuracy - 0.8736\n",
      "Epoch 5/5\n",
      "55000/55000: loss - 0.3068, accuracy - 0.8882, validation loss - 0.3916, accuracy - 0.8754\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_bottom = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    Layer12(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "])\n",
    "model_upper = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model = keras.models.Sequential([\n",
    "    model_bottom, model_upper\n",
    "])\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // (batch_size)\n",
    "loss_fn  = keras.losses.SparseCategoricalCrossentropy()\n",
    "mean_loss = keras.metrics.Mean()\n",
    "mean_acc  = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "optimizer1 = keras.optimizers.Nadam(lr=0.01)\n",
    "optimizer2 = tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    los_dic = []\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model_bottom.trainable_variables)\n",
    "        optimizer1.apply_gradients(zip(gradients, model_bottom.trainable_variables))\n",
    "        \n",
    "        gradients = tape.gradient(loss, model_upper.trainable_variables)\n",
    "        optimizer2.apply_gradients(zip(gradients, model_upper.trainable_variables))\n",
    "        \n",
    "        del gradients\n",
    "        \n",
    "        mean_loss(loss)\n",
    "        mean_acc(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), \n",
    "                         mean_loss.result().numpy(), \n",
    "                         mean_acc.result().numpy())\n",
    "        \n",
    "    y_pred   = model(X_valid)\n",
    "    val_loss = loss_fn(y_valid, y_pred)\n",
    "    val_acc  = m_accuracy(y_valid, y_pred)\n",
    "    \n",
    "    print_status_bar(len(y_train), len(y_train), \n",
    "                     mean_loss.result().numpy(), \n",
    "                     mean_acc.result().numpy(), \n",
    "                     val_loss, val_acc)\n",
    "    \n",
    "    mean_loss.reset_states()\n",
    "    mean_acc.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same result here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
